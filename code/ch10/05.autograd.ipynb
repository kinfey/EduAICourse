{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 使用“torch.autograd”自动微分\n",
        "\n",
        "在训练神经网络时，最常用的算法是\n",
        "**反向传播**。 在这个算法中，参数（模型权重）是\n",
        "根据损失函数的**梯度**进行调整\n",
        "到给定的参数。\n",
        "\n",
        "为了计算这些梯度，PyTorch 有一个内置的微分引擎\n",
        "称为“torch.autograd”。 它支持任何梯度的自动计算\n",
        "计算图。\n",
        "\n",
        "考虑最简单的一层神经网络，输入为“x”，\n",
        "参数 `w` 和 `b`，以及一些损失函数。 它可以定义在\n",
        "PyTorch 使用以下方式："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "w = torch.randn(5, 3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x, w)+b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 张量、函数和计算图\n",
        "\n",
        "此代码定义了以下**计算图**：\n",
        "\n",
        "![该图显示了一个具有两个参数 'w' 和 'b' 的计算图，用于计算损失的梯度。](./img/computational-graph.png)\n",
        "\n",
        "在这个网络中，`w` 和 `b` 是**参数**，我们需要\n",
        "优化。因此，我们需要能够计算损失的梯度\n",
        "与这些变量相关的函数。为了做到这一点，我们设置\n",
        "这些张量的 `requires_grad` 属性。\n",
        "\n",
        "> **注意：** 您可以在创建张量时设置 `requires_grad` 的值，或者稍后使用 `x.requires_grad_(True)` 方法。\n",
        "\n",
        "我们应用于张量以构建计算图的函数是\n",
        "实际上是“Function”类的对象。这个对象知道如何\n",
        "计算 *forward* 方向的函数，以及如何计算\n",
        "它在*反向传播*步骤期间的导数。参考\n",
        "反向传播函数存储在一个的 `grad_fn` 属性中\n",
        "张量。您可以在 [在\n",
        "文档]（https://pytorch.org/docs/stable/autograd.html#function）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient function for z = <AddBackward0 object at 0x7f93e0962520>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x7f93e0962280>\n"
          ]
        }
      ],
      "source": [
        "print('Gradient function for z =',z.grad_fn)\n",
        "print('Gradient function for loss =', loss.grad_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 计算梯度\n",
        "\n",
        "为了优化神经网络中参数的权重，我们需要\n",
        "计算我们的损失函数关于参数的导数，\n",
        "即，我们需要 $\\frac{\\partial loss}{\\partial w}$ 和\n",
        "$\\frac{\\partial loss}{\\partial b}$ 在一些固定值下\n",
        "`x` 和 `y`。 为了计算这些导数，我们调用\n",
        "`loss.backward()`，然后从 `w.grad` 和\n",
        "`b.grad`："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.1498, 0.0901, 0.2149],\n",
            "        [0.1498, 0.0901, 0.2149],\n",
            "        [0.1498, 0.0901, 0.2149],\n",
            "        [0.1498, 0.0901, 0.2149],\n",
            "        [0.1498, 0.0901, 0.2149]])\n",
            "tensor([0.1498, 0.0901, 0.2149])\n"
          ]
        }
      ],
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **注意：** 我们只能获取计算图的叶节点的 `grad` 属性，其 `requires_grad` 属性设置为 ``True``。 对于我们图中的所有其他节点，渐变将不可用。 此外，出于性能原因，我们只能在给定的图上使用“向后”一次执行梯度计算。 如果我们需要在同一个图上进行多次“向后”调用，我们需要将“retain_graph=True”传递给“向后”调用。\n",
        "\n",
        "## 禁用梯度跟踪\n",
        "\n",
        "默认情况下，所有带有 `requires_grad=True` 的张量都在跟踪它们的\n",
        "计算历史并支持梯度计算。 然而，有\n",
        "在某些情况下我们不需要这样做，例如，当我们有\n",
        "训练了模型，只想将其应用于某些输入数据，即我们\n",
        "只想通过网络进行*转发*计算。 我们可以停下来\n",
        "通过围绕我们的计算代码来跟踪计算\n",
        "`torch.no_grad()` 块："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another way to achieve the same result is to use the ``detach()`` method\n",
        "on the tensor:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are reasons you might want to disable gradient tracking:\n",
        "  - To mark some parameters in your neural network at **frozen parameters**. This is\n",
        "    a very common scenario for\n",
        "    [fine tuning a pre-trained network](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)\n",
        "  - To **speed up computations** when you are only doing forward pass, because computations on tensors that do\n",
        "    not track gradients would be more efficient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "More on Computational Graphs\n",
        "----------------------------\n",
        "Conceptually, autograd keeps a record of data (tensors) and all executed\n",
        "operations (along with the resulting new tensors) in a directed acyclic\n",
        "graph (DAG) consisting of\n",
        "[Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
        "objects. In this DAG, leaves are the input tensors, roots are the output\n",
        "tensors. By tracing this graph from roots to leaves, you can\n",
        "automatically compute the gradients using the chain rule.\n",
        "\n",
        "In a forward pass, autograd does two things simultaneously:\n",
        "\n",
        "- run the requested operation to compute a resulting tensor\n",
        "- maintain the operation’s *gradient function* in the DAG.\n",
        "\n",
        "The backward pass kicks off when `.backward()` is called on the DAG\n",
        "root. `autograd` then:\n",
        "\n",
        "- computes the gradients from each `.grad_fn`,\n",
        "- accumulates them in the respective tensor’s `.grad` attribute\n",
        "- using the chain rule, propagates all the way to the leaf tensors.\n",
        "\n",
        "**DAGs are dynamic in PyTorch**\n",
        "\n",
        "  An important thing to note is that the graph is recreated from scratch; after each\n",
        "  `.backward()` call, autograd starts populating a new graph. This is\n",
        "  exactly what allows you to use control flow statements in your model;\n",
        "  you can change the shape, size and operations at every iteration if\n",
        "  needed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 张量梯度和雅可比积\n",
        "\n",
        "在很多情况下，我们有一个标量损失函数，我们需要计算\n",
        "关于某些参数的梯度。不过也有案例\n",
        "当输出函数是任意张量时。在这种情况下，PyTorch\n",
        "允许您计算所谓的**雅各比积**，而不是实际的\n",
        "坡度。\n",
        "\n",
        "对于向量函数 $\\vec{y}=f(\\vec{x})$，其中\n",
        "$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ 和\n",
        "$\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$，梯度为\n",
        "$\\vec{y}$ 相对于 $\\vec{x}$ 由 **Jacobian 给出\n",
        "矩阵**：\n",
        "\n",
        "\\begin{align}\\begin{align}J=\\left(\\begin{array}{ccc}\n",
        "      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
        "      \\vdots & \\ddots & \\vdots\\\\\n",
        "      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        "      \\end{array}\\right)\\end{align}\\end{align}\n",
        "\n",
        "PyTorch 不是计算雅可比矩阵本身，而是允许您\n",
        "为给定的输入向量计算 **Jacobian Product** $v^T\\cdot J$\n",
        "$v=(v_1 \\dots v_m)$。这是通过调用 `backward` 来实现的\n",
        "$v$ 作为参数。 $v$ 的大小应该与\n",
        "原始张量的大小，我们想要\n",
        "计算产品：\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First call\n",
            " tensor([[4., 2., 2., 2., 2.],\n",
            "        [2., 4., 2., 2., 2.],\n",
            "        [2., 2., 4., 2., 2.],\n",
            "        [2., 2., 2., 4., 2.],\n",
            "        [2., 2., 2., 2., 4.]])\n",
            "\n",
            "Second call\n",
            " tensor([[8., 4., 4., 4., 4.],\n",
            "        [4., 8., 4., 4., 4.],\n",
            "        [4., 4., 8., 4., 4.],\n",
            "        [4., 4., 4., 8., 4.],\n",
            "        [4., 4., 4., 4., 8.]])\n",
            "\n",
            "Call after zeroing gradients\n",
            " tensor([[4., 2., 2., 2., 2.],\n",
            "        [2., 4., 2., 2., 2.],\n",
            "        [2., 2., 4., 2., 2.],\n",
            "        [2., 2., 2., 4., 2.],\n",
            "        [2., 2., 2., 2., 4.]])\n"
          ]
        }
      ],
      "source": [
        "inp = torch.eye(5, requires_grad=True)\n",
        "out = (inp+1).pow(2)\n",
        "out.backward(torch.ones_like(inp), retain_graph=True)\n",
        "print(\"First call\\n\", inp.grad)\n",
        "out.backward(torch.ones_like(inp), retain_graph=True)\n",
        "print(\"\\nSecond call\\n\", inp.grad)\n",
        "inp.grad.zero_()\n",
        "out.backward(torch.ones_like(inp), retain_graph=True)\n",
        "print(\"\\nCall after zeroing gradients\\n\", inp.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "请注意，当我们第二次调用 `backward` 时\n",
        "参数，梯度的值是不同的。 发生这种情况是因为\n",
        "在进行“向后”传播时，PyTorch **累积\n",
        "梯度**，即计算梯度的值被添加到\n",
        "计算图所有叶节点的`grad`属性。 如果你想\n",
        "要计算适当的梯度，您需要将 `grad` 归零\n",
        "之前的财产。 在现实生活中的训练中，*优化器* 可以帮助我们做到\n",
        "这个。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **注意：** 之前我们调用的是不带参数的 `backward()` 函数。 这等效于调用“backward(torch.tensor(1.0))”，这是在标量值函数（例如神经网络训练期间的损失）的情况下计算梯度的有用方法。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "eec8323f1fd0687cc16c170f83e009bfe267a929289f6367d1c75a585500211d"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('pydev': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
