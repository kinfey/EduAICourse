{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 处理数据\n",
        "PyTorch 有两个[处理数据的原语](https://pytorch.org/docs/stable/data.html)：“torch.utils.data.DataLoader”和“torch.utils.data.Dataset” `. ``Dataset`` 存储样本及其相应的标签，``DataLoader`` 围绕 ``Dataset`` 包装一个可迭代对象。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch 提供特定领域的库，例如 [TorchText](https://pytorch.org/text/stable/index.html)、[TorchVision](https://pytorch.org/vision/stable/index.html)、 和 [TorchAudio](https://pytorch.org/audio/stable/index.html)，所有这些都包含数据集。 在本教程中，我们将使用 TorchVision 数据集。\n",
        "\n",
        "``torchvision.datasets`` 模块包含许多真实世界视觉数据的 ``Dataset`` 对象，例如 CIFAR、COCO（[完整列表在这里]（https://pytorch.org/docs/stable/torchvision/datasets. html))。 在本教程中，我们将使用 **FashionMNIST** 数据集。 每个 TorchVision ``Dataset`` 包括两个参数：``transform`` 和 ``target_transform`` 分别用于修改样本和标签。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们将“Dataset”作为参数传递给“DataLoader”。 这在我们的数据集上包装了一个迭代，并支持自动批处理、采样、混洗和多进程数据加载。 这里我们定义了 64 的批量大小，即数据加载器迭代中的每个元素将返回一批 64 个特征和标签。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
            "Shape of y:  torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 创建模型\n",
        "为了在 PyTorch 中定义神经网络，我们创建了一个继承自 [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) 的类。 我们在 __init__ 函数中定义网络层，并在 forward 函数中指定数据如何通过网络。 为了加速神经网络中的操作，我们将其移至 GPU（如果可用）。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten()\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 优化模型参数\n",
        "为了训练模型，我们需要一个[损失函数](https://pytorch.org/docs/stable/nn.html#loss-functions>)和一个[优化器](https://pytorch.org/docs/ 稳定/optim.html）。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "在单个训练循环中，模型对训练数据集进行预测（分批提供给它），并反向传播预测误差以调整模型的参数。 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        \n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        \n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们还可以根据测试数据集检查模型的性能，以确保它正在学习。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(dataloader, model):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "训练过程经过多次迭代（*epochs*）。 在每个时期，模型学习参数以做出更好的预测。 我们在每个时期打印模型的准确性和损失； 我们希望看到准确率随着每个 epoch 的增加而减少。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.298097  [    0/60000]\n",
            "loss: 2.283960  [ 6400/60000]\n",
            "loss: 2.265772  [12800/60000]\n",
            "loss: 2.263425  [19200/60000]\n",
            "loss: 2.240961  [25600/60000]\n",
            "loss: 2.232002  [32000/60000]\n",
            "loss: 2.246419  [38400/60000]\n",
            "loss: 2.215907  [44800/60000]\n",
            "loss: 2.220379  [51200/60000]\n",
            "loss: 2.209777  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.2%, Avg loss: 0.034368 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.211811  [    0/60000]\n",
            "loss: 2.176369  [ 6400/60000]\n",
            "loss: 2.134772  [12800/60000]\n",
            "loss: 2.157168  [19200/60000]\n",
            "loss: 2.087879  [25600/60000]\n",
            "loss: 2.087876  [32000/60000]\n",
            "loss: 2.137016  [38400/60000]\n",
            "loss: 2.067152  [44800/60000]\n",
            "loss: 2.094769  [51200/60000]\n",
            "loss: 2.081813  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.6%, Avg loss: 0.031927 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.085824  [    0/60000]\n",
            "loss: 2.012798  [ 6400/60000]\n",
            "loss: 1.937523  [12800/60000]\n",
            "loss: 1.992881  [19200/60000]\n",
            "loss: 1.872041  [25600/60000]\n",
            "loss: 1.896021  [32000/60000]\n",
            "loss: 1.991944  [38400/60000]\n",
            "loss: 1.883575  [44800/60000]\n",
            "loss: 1.943997  [51200/60000]\n",
            "loss: 1.941191  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.7%, Avg loss: 0.029161 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.951519  [    0/60000]\n",
            "loss: 1.842728  [ 6400/60000]\n",
            "loss: 1.740541  [12800/60000]\n",
            "loss: 1.825037  [19200/60000]\n",
            "loss: 1.691263  [25600/60000]\n",
            "loss: 1.737593  [32000/60000]\n",
            "loss: 1.871622  [38400/60000]\n",
            "loss: 1.746524  [44800/60000]\n",
            "loss: 1.822905  [51200/60000]\n",
            "loss: 1.834781  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 51.8%, Avg loss: 0.027083 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.843257  [    0/60000]\n",
            "loss: 1.719493  [ 6400/60000]\n",
            "loss: 1.597425  [12800/60000]\n",
            "loss: 1.707815  [19200/60000]\n",
            "loss: 1.571408  [25600/60000]\n",
            "loss: 1.624887  [32000/60000]\n",
            "loss: 1.779590  [38400/60000]\n",
            "loss: 1.651785  [44800/60000]\n",
            "loss: 1.729523  [51200/60000]\n",
            "loss: 1.754649  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 53.0%, Avg loss: 0.025577 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.752509  [    0/60000]\n",
            "loss: 1.632132  [ 6400/60000]\n",
            "loss: 1.490389  [12800/60000]\n",
            "loss: 1.622813  [19200/60000]\n",
            "loss: 1.484326  [25600/60000]\n",
            "loss: 1.538245  [32000/60000]\n",
            "loss: 1.708307  [38400/60000]\n",
            "loss: 1.582635  [44800/60000]\n",
            "loss: 1.653983  [51200/60000]\n",
            "loss: 1.693552  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 53.5%, Avg loss: 0.024440 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.679617  [    0/60000]\n",
            "loss: 1.568101  [ 6400/60000]\n",
            "loss: 1.407412  [12800/60000]\n",
            "loss: 1.555965  [19200/60000]\n",
            "loss: 1.419725  [25600/60000]\n",
            "loss: 1.471584  [32000/60000]\n",
            "loss: 1.655302  [38400/60000]\n",
            "loss: 1.532972  [44800/60000]\n",
            "loss: 1.595302  [51200/60000]\n",
            "loss: 1.647840  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 53.9%, Avg loss: 0.023588 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.623256  [    0/60000]\n",
            "loss: 1.521156  [ 6400/60000]\n",
            "loss: 1.343684  [12800/60000]\n",
            "loss: 1.502695  [19200/60000]\n",
            "loss: 1.372603  [25600/60000]\n",
            "loss: 1.418489  [32000/60000]\n",
            "loss: 1.602399  [38400/60000]\n",
            "loss: 1.487690  [44800/60000]\n",
            "loss: 1.523617  [51200/60000]\n",
            "loss: 1.550696  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 57.8%, Avg loss: 0.022571 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.563997  [    0/60000]\n",
            "loss: 1.468463  [ 6400/60000]\n",
            "loss: 1.286661  [12800/60000]\n",
            "loss: 1.407763  [19200/60000]\n",
            "loss: 1.308927  [25600/60000]\n",
            "loss: 1.345932  [32000/60000]\n",
            "loss: 1.492136  [38400/60000]\n",
            "loss: 1.421576  [44800/60000]\n",
            "loss: 1.434767  [51200/60000]\n",
            "loss: 1.417038  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.6%, Avg loss: 0.021402 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.498677  [    0/60000]\n",
            "loss: 1.410989  [ 6400/60000]\n",
            "loss: 1.230321  [12800/60000]\n",
            "loss: 1.318557  [19200/60000]\n",
            "loss: 1.259680  [25600/60000]\n",
            "loss: 1.281747  [32000/60000]\n",
            "loss: 1.394046  [38400/60000]\n",
            "loss: 1.360890  [44800/60000]\n",
            "loss: 1.366970  [51200/60000]\n",
            "loss: 1.307890  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.2%, Avg loss: 0.020441 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.442546  [    0/60000]\n",
            "loss: 1.364083  [ 6400/60000]\n",
            "loss: 1.183260  [12800/60000]\n",
            "loss: 1.249731  [19200/60000]\n",
            "loss: 1.229871  [25600/60000]\n",
            "loss: 1.231694  [32000/60000]\n",
            "loss: 1.309946  [38400/60000]\n",
            "loss: 1.303778  [44800/60000]\n",
            "loss: 1.307548  [51200/60000]\n",
            "loss: 1.150738  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.2%, Avg loss: 0.018902 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.328278  [    0/60000]\n",
            "loss: 1.256034  [ 6400/60000]\n",
            "loss: 1.065482  [12800/60000]\n",
            "loss: 1.149379  [19200/60000]\n",
            "loss: 1.154137  [25600/60000]\n",
            "loss: 1.172625  [32000/60000]\n",
            "loss: 1.234418  [38400/60000]\n",
            "loss: 1.280339  [44800/60000]\n",
            "loss: 1.277301  [51200/60000]\n",
            "loss: 1.086140  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.6%, Avg loss: 0.018288 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.285433  [    0/60000]\n",
            "loss: 1.211530  [ 6400/60000]\n",
            "loss: 1.028713  [12800/60000]\n",
            "loss: 1.111656  [19200/60000]\n",
            "loss: 1.136046  [25600/60000]\n",
            "loss: 1.145143  [32000/60000]\n",
            "loss: 1.200576  [38400/60000]\n",
            "loss: 1.255864  [44800/60000]\n",
            "loss: 1.255617  [51200/60000]\n",
            "loss: 1.052424  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.8%, Avg loss: 0.017899 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.253810  [    0/60000]\n",
            "loss: 1.184758  [ 6400/60000]\n",
            "loss: 1.000494  [12800/60000]\n",
            "loss: 1.087844  [19200/60000]\n",
            "loss: 1.122644  [25600/60000]\n",
            "loss: 1.124816  [32000/60000]\n",
            "loss: 1.177433  [38400/60000]\n",
            "loss: 1.238875  [44800/60000]\n",
            "loss: 1.239779  [51200/60000]\n",
            "loss: 1.029718  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.1%, Avg loss: 0.017613 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.229939  [    0/60000]\n",
            "loss: 1.165970  [ 6400/60000]\n",
            "loss: 0.977867  [12800/60000]\n",
            "loss: 1.070287  [19200/60000]\n",
            "loss: 1.112713  [25600/60000]\n",
            "loss: 1.108338  [32000/60000]\n",
            "loss: 1.158717  [38400/60000]\n",
            "loss: 1.225702  [44800/60000]\n",
            "loss: 1.227152  [51200/60000]\n",
            "loss: 1.012273  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.4%, Avg loss: 0.017378 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 15\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "最初的准确性不会很好（没关系！）。 尝试运行循环以获得更多的“epochs”或将“learning_rate”调整为更大的数字。 也可能是我们选择的模型配置可能不是此类问题的最佳配置（事实并非如此）。 后面的课程将深入研究适用于视力问题的模型形状。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "保存模型\n",
        "-------------\n",
        "保存模型的常用方法是序列化内部状态字典（包含模型参数）。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), \"data/model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "加载模型\n",
        "-----------------------------\n",
        "\n",
        "加载模型的过程包括重新创建模型结构和加载\n",
        "将状态字典放入其中。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = NeuralNetwork()\n",
        "model.load_state_dict(torch.load(\"data/model.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "该模型现在可用于进行预测。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
          ]
        }
      ],
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "eec8323f1fd0687cc16c170f83e009bfe267a929289f6367d1c75a585500211d"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('pydev': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
