{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归\n",
    "\n",
    "线性回归是机器学习中的基本算法之一。 线性回归在输入特征 (X) 和输出标签 (y) 之间建立线性关系。\n",
    "\n",
    "在线性回归中，每个输出标签都表示为使用权重和偏差的输入特征的线性函数。 这些权重和偏差是随机初始化的模型参数，然后通过数据集的每个训练/学习周期进行更新。 在经过一次训练数据迭代后训练模型和更新参数被称为一个时期。 所以现在我们应该训练模型几个时期，以便权重和偏差可以学习输入特征和输出标签之间的线性关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子我们将根据特定地点的平均温度、年降雨量和湿度，创建一个由芒果和橙子的作物产量组成的假设数据模型。\n",
    "\n",
    "Mangoes = w11 * temp + w12 * rainfall + w13 * humidity + b1 \n",
    "\n",
    "Oranges = w21 * temp + w22 * rainfall + w23 * humidity + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70], \n",
    "                   [74, 66, 43], \n",
    "                   [91, 87, 65], \n",
    "                   [88, 134, 59], \n",
    "                   [101, 44, 37], \n",
    "                   [68, 96, 71], \n",
    "                   [73, 66, 44], \n",
    "                   [92, 87, 64], \n",
    "                   [87, 135, 57], \n",
    "                   [103, 43, 36], \n",
    "                   [68, 97, 70]], \n",
    "                  dtype='float32')\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119],\n",
    "                    [57, 69], \n",
    "                    [80, 102], \n",
    "                    [118, 132], \n",
    "                    [21, 38], \n",
    "                    [104, 118], \n",
    "                    [57, 69], \n",
    "                    [82, 100], \n",
    "                    [118, 134], \n",
    "                    [20, 38], \n",
    "                    [102, 120]], \n",
    "                   dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.],\n",
      "        [ 74.,  66.,  43.],\n",
      "        [ 91.,  87.,  65.],\n",
      "        [ 88., 134.,  59.],\n",
      "        [101.,  44.,  37.],\n",
      "        [ 68.,  96.,  71.],\n",
      "        [ 73.,  66.,  44.],\n",
      "        [ 92.,  87.,  64.],\n",
      "        [ 87., 135.,  57.],\n",
      "        [103.,  43.,  36.],\n",
      "        [ 68.,  97.,  70.]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.],\n",
      "        [ 57.,  69.],\n",
      "        [ 80., 102.],\n",
      "        [118., 132.],\n",
      "        [ 21.,  38.],\n",
      "        [104., 118.],\n",
      "        [ 57.,  69.],\n",
      "        [ 82., 100.],\n",
      "        [118., 134.],\n",
      "        [ 20.,  38.],\n",
      "        [102., 120.]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "dataset = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 3\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 91.,  88.,  64.],\n",
      "        [ 68.,  96.,  71.],\n",
      "        [ 87., 135.,  57.]])\n",
      "tensor([[ 81., 101.],\n",
      "        [104., 118.],\n",
      "        [118., 134.]])\n"
     ]
    }
   ],
   "source": [
    "# A Batch Sample\n",
    "for inp,target in train_loader:\n",
    "    print(inp)\n",
    "    print(target)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从头开始实现一个线性回归模型。 我们应该找到上面等式中指定的最佳权重和偏差，以便它定义输入和输出之间的理想线性关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4496e+00, -3.3164e-01, -1.4695e-03],\n",
      "        [-1.9847e+00, -7.8420e-01,  4.0560e-03]], requires_grad=True)\n",
      "tensor([-0.6681, -0.2995], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X):\n",
    "    return X @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is :n tensor([[  83.1969, -196.7641],\n",
      "        [ 102.2937, -248.8724],\n",
      "        [  65.9602, -210.2570]], grad_fn=<AddBackward0>)\n",
      "nActual targets is :n tensor([[ 57.,  69.],\n",
      "        [ 80., 102.],\n",
      "        [104., 118.]])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    preds = model(x)\n",
    "    print(\"Prediction is :n\",preds)\n",
    "    print(\"nActual targets is :n\",y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(predictions, targets):\n",
    "    difference = predictions - targets\n",
    "    return torch.sum(difference * difference)/ difference.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is :n tensor([[  80.5884, -278.6077],\n",
      "        [  67.4112, -212.2458],\n",
      "        [ 132.8723, -236.3132]], grad_fn=<AddBackward0>)\n",
      "nActual targets is :n tensor([[118., 134.],\n",
      "        [103., 119.],\n",
      "        [ 22.,  37.]])\n",
      "nLoss is:  tensor(61604.6406, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    preds = model(x)\n",
    "    print(\"Prediction is :n\",preds)\n",
    "    print(\"nActual targets is :n\",y)\n",
    "    print(\"nLoss is: \",mse_loss(preds, y))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50: Loss: 61412.234375\n",
      "Epoch 0/50: Loss: 49319.40625\n",
      "Epoch 0/50: Loss: 52399.94921875\n",
      "Epoch 0/50: Loss: 47296.625\n",
      "Epoch 0/50: Loss: 53077.19140625\n",
      "Epoch 1/50: Loss: 42723.45703125\n",
      "Epoch 1/50: Loss: 38543.38671875\n",
      "Epoch 1/50: Loss: 59146.80078125\n",
      "Epoch 1/50: Loss: 33769.9765625\n",
      "Epoch 1/50: Loss: 47197.15625\n",
      "Epoch 2/50: Loss: 36682.85546875\n",
      "Epoch 2/50: Loss: 46485.5625\n",
      "Epoch 2/50: Loss: 38457.91015625\n",
      "Epoch 2/50: Loss: 32818.33984375\n",
      "Epoch 2/50: Loss: 31486.59375\n",
      "Epoch 3/50: Loss: 33356.796875\n",
      "Epoch 3/50: Loss: 26187.927734375\n",
      "Epoch 3/50: Loss: 31339.9609375\n",
      "Epoch 3/50: Loss: 34430.23046875\n",
      "Epoch 3/50: Loss: 31001.322265625\n",
      "Epoch 4/50: Loss: 29939.220703125\n",
      "Epoch 4/50: Loss: 30117.255859375\n",
      "Epoch 4/50: Loss: 25905.005859375\n",
      "Epoch 4/50: Loss: 22205.5234375\n",
      "Epoch 4/50: Loss: 23500.169921875\n",
      "Epoch 5/50: Loss: 20402.265625\n",
      "Epoch 5/50: Loss: 20036.771484375\n",
      "Epoch 5/50: Loss: 25021.177734375\n",
      "Epoch 5/50: Loss: 27657.431640625\n",
      "Epoch 5/50: Loss: 18041.07421875\n",
      "Epoch 6/50: Loss: 17318.986328125\n",
      "Epoch 6/50: Loss: 22415.59375\n",
      "Epoch 6/50: Loss: 14092.5546875\n",
      "Epoch 6/50: Loss: 17674.8125\n",
      "Epoch 6/50: Loss: 22671.267578125\n",
      "Epoch 7/50: Loss: 15053.5810546875\n",
      "Epoch 7/50: Loss: 16416.978515625\n",
      "Epoch 7/50: Loss: 20055.681640625\n",
      "Epoch 7/50: Loss: 15189.7001953125\n",
      "Epoch 7/50: Loss: 13016.8564453125\n",
      "Epoch 8/50: Loss: 16580.853515625\n",
      "Epoch 8/50: Loss: 14399.6328125\n",
      "Epoch 8/50: Loss: 12711.8310546875\n",
      "Epoch 8/50: Loss: 11171.8681640625\n",
      "Epoch 8/50: Loss: 12804.9970703125\n",
      "Epoch 9/50: Loss: 10591.546875\n",
      "Epoch 9/50: Loss: 15163.7978515625\n",
      "Epoch 9/50: Loss: 10397.4189453125\n",
      "Epoch 9/50: Loss: 10807.171875\n",
      "Epoch 9/50: Loss: 10762.916015625\n",
      "Epoch 10/50: Loss: 11155.16015625\n",
      "Epoch 10/50: Loss: 9873.5390625\n",
      "Epoch 10/50: Loss: 10754.5341796875\n",
      "Epoch 10/50: Loss: 9347.84765625\n",
      "Epoch 10/50: Loss: 8244.841796875\n",
      "Epoch 11/50: Loss: 10476.2060546875\n",
      "Epoch 11/50: Loss: 5860.34716796875\n",
      "Epoch 11/50: Loss: 6568.81787109375\n",
      "Epoch 11/50: Loss: 10025.0048828125\n",
      "Epoch 11/50: Loss: 9435.0302734375\n",
      "Epoch 12/50: Loss: 6052.93505859375\n",
      "Epoch 12/50: Loss: 6781.68994140625\n",
      "Epoch 12/50: Loss: 7977.41015625\n",
      "Epoch 12/50: Loss: 8637.7373046875\n",
      "Epoch 12/50: Loss: 7164.06494140625\n",
      "Epoch 13/50: Loss: 7783.77099609375\n",
      "Epoch 13/50: Loss: 6835.35693359375\n",
      "Epoch 13/50: Loss: 6798.296875\n",
      "Epoch 13/50: Loss: 5750.265625\n",
      "Epoch 13/50: Loss: 4536.10107421875\n",
      "Epoch 14/50: Loss: 5882.78125\n",
      "Epoch 14/50: Loss: 4737.345703125\n",
      "Epoch 14/50: Loss: 4081.70703125\n",
      "Epoch 14/50: Loss: 6582.30859375\n",
      "Epoch 14/50: Loss: 6332.58251953125\n",
      "Epoch 15/50: Loss: 4466.9228515625\n",
      "Epoch 15/50: Loss: 3548.621826171875\n",
      "Epoch 15/50: Loss: 6664.35693359375\n",
      "Epoch 15/50: Loss: 4428.29248046875\n",
      "Epoch 15/50: Loss: 5125.599609375\n",
      "Epoch 16/50: Loss: 2687.711181640625\n",
      "Epoch 16/50: Loss: 5274.44580078125\n",
      "Epoch 16/50: Loss: 4030.25\n",
      "Epoch 16/50: Loss: 3441.501708984375\n",
      "Epoch 16/50: Loss: 5969.53271484375\n",
      "Epoch 17/50: Loss: 4616.115234375\n",
      "Epoch 17/50: Loss: 2239.614990234375\n",
      "Epoch 17/50: Loss: 4135.26953125\n",
      "Epoch 17/50: Loss: 3488.274658203125\n",
      "Epoch 17/50: Loss: 4509.70068359375\n",
      "Epoch 18/50: Loss: 4342.74365234375\n",
      "Epoch 18/50: Loss: 3560.426025390625\n",
      "Epoch 18/50: Loss: 2571.841552734375\n",
      "Epoch 18/50: Loss: 2644.116455078125\n",
      "Epoch 18/50: Loss: 3853.222900390625\n",
      "Epoch 19/50: Loss: 5200.0693359375\n",
      "Epoch 19/50: Loss: 2553.9912109375\n",
      "Epoch 19/50: Loss: 3453.189453125\n",
      "Epoch 19/50: Loss: 2275.194091796875\n",
      "Epoch 19/50: Loss: 1832.2373046875\n",
      "Epoch 20/50: Loss: 3462.014892578125\n",
      "Epoch 20/50: Loss: 3315.672607421875\n",
      "Epoch 20/50: Loss: 1768.8214111328125\n",
      "Epoch 20/50: Loss: 3215.171142578125\n",
      "Epoch 20/50: Loss: 2143.493408203125\n",
      "Epoch 21/50: Loss: 1484.4569091796875\n",
      "Epoch 21/50: Loss: 2896.8740234375\n",
      "Epoch 21/50: Loss: 4581.03662109375\n",
      "Epoch 21/50: Loss: 2036.5738525390625\n",
      "Epoch 21/50: Loss: 1750.7540283203125\n",
      "Epoch 22/50: Loss: 1153.05908203125\n",
      "Epoch 22/50: Loss: 1975.8304443359375\n",
      "Epoch 22/50: Loss: 2914.162109375\n",
      "Epoch 22/50: Loss: 2941.047607421875\n",
      "Epoch 22/50: Loss: 2751.437744140625\n",
      "Epoch 23/50: Loss: 749.2672729492188\n",
      "Epoch 23/50: Loss: 1618.7078857421875\n",
      "Epoch 23/50: Loss: 2797.8291015625\n",
      "Epoch 23/50: Loss: 1485.6282958984375\n",
      "Epoch 23/50: Loss: 4264.75927734375\n",
      "Epoch 24/50: Loss: 2927.075927734375\n",
      "Epoch 24/50: Loss: 1351.5765380859375\n",
      "Epoch 24/50: Loss: 1051.6717529296875\n",
      "Epoch 24/50: Loss: 1003.6902465820312\n",
      "Epoch 24/50: Loss: 3886.996337890625\n",
      "Epoch 25/50: Loss: 2964.904052734375\n",
      "Epoch 25/50: Loss: 919.4013671875\n",
      "Epoch 25/50: Loss: 1120.8345947265625\n",
      "Epoch 25/50: Loss: 4091.688720703125\n",
      "Epoch 25/50: Loss: 514.7432250976562\n",
      "Epoch 26/50: Loss: 2471.33056640625\n",
      "Epoch 26/50: Loss: 2184.350341796875\n",
      "Epoch 26/50: Loss: 1422.1219482421875\n",
      "Epoch 26/50: Loss: 2207.496337890625\n",
      "Epoch 26/50: Loss: 836.5239868164062\n",
      "Epoch 27/50: Loss: 470.8787841796875\n",
      "Epoch 27/50: Loss: 2345.35400390625\n",
      "Epoch 27/50: Loss: 956.0234985351562\n",
      "Epoch 27/50: Loss: 2662.950439453125\n",
      "Epoch 27/50: Loss: 2236.4228515625\n",
      "Epoch 28/50: Loss: 763.6383666992188\n",
      "Epoch 28/50: Loss: 2369.795654296875\n",
      "Epoch 28/50: Loss: 3358.179443359375\n",
      "Epoch 28/50: Loss: 965.3712768554688\n",
      "Epoch 28/50: Loss: 885.2743530273438\n",
      "Epoch 29/50: Loss: 2236.595703125\n",
      "Epoch 29/50: Loss: 960.2649536132812\n",
      "Epoch 29/50: Loss: 685.642578125\n",
      "Epoch 29/50: Loss: 2090.57763671875\n",
      "Epoch 29/50: Loss: 2032.0592041015625\n",
      "Epoch 30/50: Loss: 345.7877502441406\n",
      "Epoch 30/50: Loss: 2098.83642578125\n",
      "Epoch 30/50: Loss: 2276.5234375\n",
      "Epoch 30/50: Loss: 891.053955078125\n",
      "Epoch 30/50: Loss: 2128.823974609375\n",
      "Epoch 31/50: Loss: 2192.395751953125\n",
      "Epoch 31/50: Loss: 1734.4757080078125\n",
      "Epoch 31/50: Loss: 1949.34765625\n",
      "Epoch 31/50: Loss: 783.4768676757812\n",
      "Epoch 31/50: Loss: 868.84228515625\n",
      "Epoch 32/50: Loss: 505.0893249511719\n",
      "Epoch 32/50: Loss: 1124.4825439453125\n",
      "Epoch 32/50: Loss: 274.0461730957031\n",
      "Epoch 32/50: Loss: 3291.878173828125\n",
      "Epoch 32/50: Loss: 2164.435791015625\n",
      "Epoch 33/50: Loss: 1881.1304931640625\n",
      "Epoch 33/50: Loss: 1791.3837890625\n",
      "Epoch 33/50: Loss: 1899.8424072265625\n",
      "Epoch 33/50: Loss: 760.4725952148438\n",
      "Epoch 33/50: Loss: 832.5579223632812\n",
      "Epoch 34/50: Loss: 2045.4197998046875\n",
      "Epoch 34/50: Loss: 691.0239868164062\n",
      "Epoch 34/50: Loss: 707.3097534179688\n",
      "Epoch 34/50: Loss: 1920.03076171875\n",
      "Epoch 34/50: Loss: 1647.0306396484375\n",
      "Epoch 35/50: Loss: 726.4541625976562\n",
      "Epoch 35/50: Loss: 2013.1842041015625\n",
      "Epoch 35/50: Loss: 3015.070068359375\n",
      "Epoch 35/50: Loss: 440.813232421875\n",
      "Epoch 35/50: Loss: 702.5294799804688\n",
      "Epoch 36/50: Loss: 2178.2998046875\n",
      "Epoch 36/50: Loss: 1882.4757080078125\n",
      "Epoch 36/50: Loss: 1801.4232177734375\n",
      "Epoch 36/50: Loss: 170.19166564941406\n",
      "Epoch 36/50: Loss: 719.0364379882812\n",
      "Epoch 37/50: Loss: 2949.261962890625\n",
      "Epoch 37/50: Loss: 469.7037048339844\n",
      "Epoch 37/50: Loss: 390.6261901855469\n",
      "Epoch 37/50: Loss: 692.0747680664062\n",
      "Epoch 37/50: Loss: 2171.39013671875\n",
      "Epoch 38/50: Loss: 3037.09375\n",
      "Epoch 38/50: Loss: 438.87353515625\n",
      "Epoch 38/50: Loss: 2054.638671875\n",
      "Epoch 38/50: Loss: 375.3104248046875\n",
      "Epoch 38/50: Loss: 674.786376953125\n",
      "Epoch 39/50: Loss: 1806.2236328125\n",
      "Epoch 39/50: Loss: 2138.948486328125\n",
      "Epoch 39/50: Loss: 1724.662109375\n",
      "Epoch 39/50: Loss: 385.0294494628906\n",
      "Epoch 39/50: Loss: 410.9169921875\n",
      "Epoch 40/50: Loss: 667.2129516601562\n",
      "Epoch 40/50: Loss: 2088.03759765625\n",
      "Epoch 40/50: Loss: 1777.6722412109375\n",
      "Epoch 40/50: Loss: 1492.4017333984375\n",
      "Epoch 40/50: Loss: 380.0084533691406\n",
      "Epoch 41/50: Loss: 1817.0367431640625\n",
      "Epoch 41/50: Loss: 1470.9376220703125\n",
      "Epoch 41/50: Loss: 687.2752075195312\n",
      "Epoch 41/50: Loss: 367.8781433105469\n",
      "Epoch 41/50: Loss: 1994.287109375\n",
      "Epoch 42/50: Loss: 84.30628204345703\n",
      "Epoch 42/50: Loss: 663.4564819335938\n",
      "Epoch 42/50: Loss: 1729.08154296875\n",
      "Epoch 42/50: Loss: 1719.3800048828125\n",
      "Epoch 42/50: Loss: 2069.495361328125\n",
      "Epoch 43/50: Loss: 640.1018676757812\n",
      "Epoch 43/50: Loss: 410.4323425292969\n",
      "Epoch 43/50: Loss: 2783.852783203125\n",
      "Epoch 43/50: Loss: 2059.74072265625\n",
      "Epoch 43/50: Loss: 331.4644470214844\n",
      "Epoch 44/50: Loss: 375.2243347167969\n",
      "Epoch 44/50: Loss: 1900.41455078125\n",
      "Epoch 44/50: Loss: 1692.6827392578125\n",
      "Epoch 44/50: Loss: 1571.2569580078125\n",
      "Epoch 44/50: Loss: 615.5901489257812\n",
      "Epoch 45/50: Loss: 2069.194091796875\n",
      "Epoch 45/50: Loss: 369.7217102050781\n",
      "Epoch 45/50: Loss: 1633.5260009765625\n",
      "Epoch 45/50: Loss: 1674.15087890625\n",
      "Epoch 45/50: Loss: 342.3143615722656\n",
      "Epoch 46/50: Loss: 1651.4232177734375\n",
      "Epoch 46/50: Loss: 1764.8897705078125\n",
      "Epoch 46/50: Loss: 614.8169555664062\n",
      "Epoch 46/50: Loss: 1655.54345703125\n",
      "Epoch 46/50: Loss: 357.823486328125\n",
      "Epoch 47/50: Loss: 363.3150939941406\n",
      "Epoch 47/50: Loss: 338.5794372558594\n",
      "Epoch 47/50: Loss: 2000.1099853515625\n",
      "Epoch 47/50: Loss: 2712.061767578125\n",
      "Epoch 47/50: Loss: 606.4036865234375\n",
      "Epoch 48/50: Loss: 617.6802368164062\n",
      "Epoch 48/50: Loss: 353.71240234375\n",
      "Epoch 48/50: Loss: 1755.5638427734375\n",
      "Epoch 48/50: Loss: 2903.841552734375\n",
      "Epoch 48/50: Loss: 331.8881530761719\n",
      "Epoch 49/50: Loss: 308.0274353027344\n",
      "Epoch 49/50: Loss: 1647.314453125\n",
      "Epoch 49/50: Loss: 604.2324829101562\n",
      "Epoch 49/50: Loss: 1624.2288818359375\n",
      "Epoch 49/50: Loss: 1723.9581298828125\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "for i in range(epochs):\n",
    "    # Iterate through training dataloader\n",
    "    for x,y in train_loader:\n",
    "        # Generate Prediction\n",
    "        preds = model(x)\n",
    "        # Get the loss and perform backpropagation\n",
    "        loss = mse_loss(preds, y)\n",
    "        loss.backward()\n",
    "        # Let's update the weights\n",
    "        with torch.no_grad():\n",
    "            w -= w.grad *1e-6\n",
    "            b -= b.grad * 1e-6\n",
    "            # Set the gradients to zero\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "        print(f\"Epoch {i}/{epochs}: Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is :n tensor([[ 89.7322,  96.8692],\n",
      "        [ 61.7146, 120.6202],\n",
      "        [ 90.8762,  95.4166]], grad_fn=<AddBackward0>)\n",
      "nActual targets is :n tensor([[ 80., 102.],\n",
      "        [104., 118.],\n",
      "        [ 82., 100.]])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    preds = model(x)\n",
    "    print(\"Prediction is :n\",preds)\n",
    "    print(\"nActual targets is :n\",y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82f1cf4c281cab90794d57a9753ced65abcf10e305350b912a77a2456f8d88f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pydev': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
