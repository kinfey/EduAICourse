{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 表示文本\n",
        "\n",
        "如果我们想用神经网络解决自然语言处理 (NLP) 任务，我们需要某种方式将文本表示为张量。计算机已经将文本字符表示为使用 ASCII 或 UTF-8 等编码映射到屏幕上字体的数字。\n",
        "\n",
        "![显示将字符映射到 ASCII 和二进制表示的图表的图像](./images/ascii-character-map.png)\n",
        "\n",
        "我们了解每个字母 ** 代表** 的含义，以及所有字符如何组合在一起形成句子的单词。但是，计算机本身并没有这样的理解，神经网络必须在训练过程中学习其含义。\n",
        "\n",
        "因此，我们可以在表示文本时使用不同的方法：\n",
        "* **字符级表示**，当我们通过将每个字符视为数字来表示文本时。鉴于我们的文本语料库中有 $C$ 不同的字符，单词 *Hello* 将由 $5\\times C$ 张量表示。每个字母将对应于 one-hot 编码中的一个张量列。\n",
        "* **词级表示**，其中我们为文本中的所有单词创建一个**词汇表**，然后使用one-hot编码表示单词。这种方法在某种程度上更好，因为每个字母本身并没有多大意义，因此通过使用更高级别的语义概念——单词——我们简化了神经网络的任务。然而，鉴于字典很大，我们需要处理高维稀疏张量。\n",
        "\n",
        "让我们首先安装一些我们将在本模块中使用的必需 Python 包。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting gensim==3.8.3\n",
            "  Using cached gensim-3.8.3-cp38-cp38-macosx_10_9_x86_64.whl (24.2 MB)\n",
            "Collecting huggingface==0.0.1\n",
            "  Using cached huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.5.1-cp38-cp38-macosx_10_9_x86_64.whl (7.3 MB)\n",
            "     |████████████████████████████████| 7.3 MB 1.5 MB/s            \n",
            "\u001b[?25hCollecting nltk==3.5\n",
            "  Using cached nltk-3.5-py3-none-any.whl\n",
            "Collecting numpy==1.18.5\n",
            "  Using cached numpy-1.18.5-cp38-cp38-macosx_10_9_x86_64.whl (15.1 MB)\n",
            "Collecting opencv-python==4.5.1.48\n",
            "  Using cached opencv_python-4.5.1.48-cp38-cp38-macosx_10_13_x86_64.whl (40.3 MB)\n",
            "Collecting Pillow==7.1.2\n",
            "  Using cached Pillow-7.1.2-cp38-cp38-macosx_10_10_x86_64.whl (2.2 MB)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.0.1-cp38-cp38-macosx_10_13_x86_64.whl (7.9 MB)\n",
            "     |████████████████████████████████| 7.9 MB 2.7 MB/s            \n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.7.3-cp38-cp38-macosx_10_9_x86_64.whl (33.0 MB)\n",
            "     |████████████████████████████████| 33.0 MB 3.2 MB/s            \n",
            "\u001b[?25hCollecting torch==1.8.1\n",
            "  Using cached torch-1.8.1-cp38-none-macosx_10_9_x86_64.whl (119.6 MB)\n",
            "Collecting torchaudio==0.8.1\n",
            "  Using cached torchaudio-0.8.1-cp38-cp38-macosx_10_9_x86_64.whl (1.5 MB)\n",
            "Collecting torchinfo==0.0.8\n",
            "  Using cached torchinfo-0.0.8-py3-none-any.whl (16 kB)\n",
            "Collecting torchtext==0.9.1\n",
            "  Using cached torchtext-0.9.1-cp38-cp38-macosx_10_9_x86_64.whl (1.6 MB)\n",
            "Collecting torchvision==0.9.1\n",
            "  Using cached torchvision-0.9.1-cp38-cp38-macosx_10_9_x86_64.whl (13.2 MB)\n",
            "Collecting transformers==4.3.3\n",
            "  Using cached transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
            "Requirement already satisfied: six>=1.5.0 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages (from gensim==3.8.3->-r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt (line 1)) (1.15.0)\n",
            "Collecting smart-open>=1.8.1\n",
            "  Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "Collecting joblib\n",
            "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "     |████████████████████████████████| 306 kB 3.0 MB/s            \n",
            "\u001b[?25hCollecting click\n",
            "  Using cached click-8.0.3-py3-none-any.whl (97 kB)\n",
            "Collecting tqdm\n",
            "  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
            "Collecting regex\n",
            "  Using cached regex-2021.11.10-cp38-cp38-macosx_10_9_x86_64.whl (288 kB)\n",
            "Collecting typing-extensions\n",
            "  Downloading typing_extensions-4.0.1-py3-none-any.whl (22 kB)\n",
            "Collecting requests\n",
            "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Using cached tokenizers-0.10.3-cp38-cp38-macosx_10_11_x86_64.whl (2.2 MB)\n",
            "Collecting packaging\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "     |████████████████████████████████| 40 kB 4.1 MB/s            \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Using cached sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "Collecting filelock\n",
            "  Using cached filelock-3.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.28.5-py3-none-any.whl (890 kB)\n",
            "     |████████████████████████████████| 890 kB 4.1 MB/s            \n",
            "\u001b[?25hCollecting cycler>=0.10\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl (61 kB)\n",
            "     |████████████████████████████████| 61 kB 436 kB/s            \n",
            "\u001b[?25hCollecting python-dateutil>=2.7\n",
            "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "Collecting pyparsing>=2.2.1\n",
            "  Downloading pyparsing-3.0.6-py3-none-any.whl (97 kB)\n",
            "     |████████████████████████████████| 97 kB 4.2 MB/s            \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting charset-normalizer~=2.0.0\n",
            "  Downloading charset_normalizer-2.0.9-py3-none-any.whl (39 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Using cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
            "Installing collected packages: urllib3, typing-extensions, tqdm, regex, pyparsing, numpy, joblib, idna, click, charset-normalizer, certifi, torch, tokenizers, threadpoolctl, smart-open, scipy, sacremoses, requests, python-dateutil, Pillow, packaging, kiwisolver, fonttools, filelock, cycler, transformers, torchvision, torchtext, torchinfo, torchaudio, scikit-learn, opencv-python, nltk, matplotlib, huggingface, gensim\n",
            "\u001b[33m  WARNING: The script tqdm is installed in '/Users/lokinfey/Library/Python/3.8/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.8 are installed in '/Users/lokinfey/Library/Python/3.8/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script normalizer is installed in '/Users/lokinfey/Library/Python/3.8/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/Users/lokinfey/Library/Python/3.8/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script sacremoses is installed in '/Users/lokinfey/Library/Python/3.8/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts fonttools, pyftmerge, pyftsubset and ttx are installed in '/Users/lokinfey/Library/Python/3.8/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script transformers-cli is installed in '/Users/lokinfey/Library/Python/3.8/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script nltk is installed in '/Users/lokinfey/Library/Python/3.8/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed Pillow-7.1.2 certifi-2021.10.8 charset-normalizer-2.0.9 click-8.0.3 cycler-0.11.0 filelock-3.4.0 fonttools-4.28.5 gensim-3.8.3 huggingface-0.0.1 idna-3.3 joblib-1.1.0 kiwisolver-1.3.2 matplotlib-3.5.1 nltk-3.5 numpy-1.18.5 opencv-python-4.5.1.48 packaging-21.3 pyparsing-3.0.6 python-dateutil-2.8.2 regex-2021.11.10 requests-2.26.0 sacremoses-0.0.46 scikit-learn-1.0.1 scipy-1.7.3 smart-open-5.2.1 threadpoolctl-3.0.0 tokenizers-0.10.3 torch-1.8.1 torchaudio-0.8.1 torchinfo-0.0.8 torchtext-0.9.1 torchvision-0.9.1 tqdm-4.62.3 transformers-4.3.3 typing-extensions-4.0.1 urllib3-1.26.7\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/nlp-pytorch/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 文本分类任务\n",
        "\n",
        "在本模块中，我们将从基于 **AG_NEWS** 数据集的简单文本分类任务开始，该任务将新闻标题分类为 4 个类别之一：World、Sports、Business 和 Sci/Tech。 该数据集内置于 [`torchtext`](https://github.com/pytorch/text) 模块中，因此我们可以轻松访问它。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train.csv: 29.5MB [00:04, 6.29MB/s]                            \n",
            "test.csv: 1.86MB [00:00, 4.27MB/s]                          \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import os\n",
        "import collections\n",
        "os.makedirs('./data',exist_ok=True)\n",
        "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
        "classes = ['World', 'Sports', 'Business', 'Sci/Tech']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这里，`train_dataset` 和 `test_dataset` 包含迭代器，它们分别返回标签对（类别数）和文本，例如："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3,\n",
              " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "因此，让我们打印出数据集中的前 10 个新标题："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Sci/Tech** -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
            "**Sci/Tech** -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
            "**Sci/Tech** -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
            "**Sci/Tech** -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n",
            "**Sci/Tech** -> Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\but stayed near lows for the year as oil prices surged past  #36;46\\a barrel, offsetting a positive outlook from computer maker\\Dell Inc. (DELL.O)\n"
          ]
        }
      ],
      "source": [
        "for i,x in zip(range(5),train_dataset):\n",
        "    print(f\"**{classes[x[0]]}** -> {x[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "因为数据集是迭代器，如果我们想多次使用数据，我们需要将其转换为列表："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(root='./data')\n",
        "train_dataset = list(train_dataset)\n",
        "test_dataset = list(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在我们需要将文本转换为可以表示为张量的**数字**。 如果我们想要词级表示，我们需要做两件事：\n",
        "* 使用 **tokenizer** 将文本拆分为 **tokens**\n",
        "* 构建这些标记的 **词汇**。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['he', 'said', 'hello']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
        "tokenizer('He said: hello')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "counter = collections.Counter()\n",
        "for (label, line) in train_dataset:\n",
        "    counter.update(tokenizer(line))\n",
        "vocab = torchtext.vocab.Vocab(counter, min_freq=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "使用词汇表，我们可以轻松地将标记化的字符串编码为一组数字："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size if 95812\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[283, 2321, 5, 337, 19, 1301, 2357]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = len(vocab)\n",
        "print(f\"Vocab size if {vocab_size}\")\n",
        "\n",
        "def encode(x):\n",
        "    return [vocab.stoi[s] for s in tokenizer(x)]\n",
        "\n",
        "encode('I love to play with my words')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 词袋文本表示\n",
        "\n",
        "因为单词代表意义，有时我们可以通过查看单个单词来理解文本的含义，而不管它们在句子中的顺序如何。 例如，在对新闻进行分类时，像*weather*、*snow* 这样的词可能表示*weather forecast*，而像*stocks*、*dollar* 这样的词将计入*financial news*。\n",
        "\n",
        "**Bag of Words** (BoW) 向量表示是最常用的传统向量表示。 每个词都链接到一个向量索引，向量元素包含一个词在给定文档中出现的次数。\n",
        "\n",
        "![图像显示了词袋向量表示如何在内存中表示。](./images/bag-of-words-example.png)\n",
        "\n",
        "> **注意**：您还可以将 BoW 视为文本中单个单词的所有单热编码向量的总和。\n",
        "\n",
        "下面是如何使用 Scikit Learn python 库生成词袋表示的示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "corpus = [\n",
        "        'I like hot dogs.',\n",
        "        'The dog ran fast.',\n",
        "        'Its hot outside.',\n",
        "    ]\n",
        "vectorizer.fit_transform(corpus)\n",
        "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "要从 AG_NEWS 数据集的向量表示中计算词袋向量，我们可以使用以下函数："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 2.,  ..., 0., 0., 0.])\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "def to_bow(text,bow_vocab_size=vocab_size):\n",
        "    res = torch.zeros(bow_vocab_size,dtype=torch.float32)\n",
        "    for i in encode(text):\n",
        "        if i<bow_vocab_size:\n",
        "            res[i] += 1\n",
        "    return res\n",
        "\n",
        "print(to_bow(train_dataset[0][1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **注意：** 这里我们使用全局`vocab_size` 变量来指定词汇表的默认大小。 由于通常词汇量非常大，我们可以将词汇量限制为最常用的词。 尝试降低 `vocab_size` 值并运行下面的代码，看看它如何影响准确性。 您应该期待一些准确度下降，但不会显着下降，以代替更高的性能。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 训练 BoW 分类器\n",
        "\n",
        "现在我们已经学会了如何构建文本的词袋表示，让我们在它之上训练一个分类器。 首先，我们需要以这种方式转换我们的训练数据集，将所有位置向量表示转换为词袋表示。 这可以通过将 `bowify` 函数作为 `collate_fn` 参数传递给标准 Torch `DataLoader` 来实现："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在让我们定义一个包含一个线性层的简单分类器神经网络。 输入向量的大小等于`vocab_size`，输出大小对应于类的数量（4）。 因为我们是在解决分类任务，所以最终的激活函数是`LogSoftmax()`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import numpy as np \n",
        "\n",
        "# this collate function gets list of batch_size tuples, and needs to \n",
        "# return a pair of label-feature tensors for the whole minibatch\n",
        "def bowify(b):\n",
        "    return (\n",
        "            torch.LongTensor([t[0]-1 for t in b]),\n",
        "            torch.stack([to_bow(t[1]) for t in b])\n",
        "    )\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=bowify, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=bowify, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在让我们定义一个包含一个线性层的简单分类器神经网络。 输入向量的大小等于`vocab_size`，输出大小对应于类的数量（4）。 因为我们是在解决分类任务，所以最终的激活函数是`LogSoftmax()`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = torch.nn.Sequential(torch.nn.Linear(vocab_size,4),torch.nn.LogSoftmax(dim=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在我们将定义标准的 PyTorch 训练循环。 因为我们的数据集非常大，出于教学目的，我们只会训练一个 epoch，有时甚至少于一个 epoch（指定 epoch_size 参数允许我们限制训练）。 我们还会在训练过程中报告累积的训练准确率； 报告频率是使用`report_freq` 参数指定的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(net,dataloader,lr=0.01,optimizer=None,loss_fn = torch.nn.NLLLoss(),epoch_size=None, report_freq=200):\n",
        "    optimizer = optimizer or torch.optim.Adam(net.parameters(),lr=lr)\n",
        "    net.train()\n",
        "    total_loss,acc,count,i = 0,0,0,0\n",
        "    for labels,features in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        out = net(features)\n",
        "        loss = loss_fn(out,labels) #cross_entropy(out,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss+=loss\n",
        "        _,predicted = torch.max(out,1)\n",
        "        acc+=(predicted==labels).sum()\n",
        "        count+=len(labels)\n",
        "        i+=1\n",
        "        if i%report_freq==0:\n",
        "            print(f\"{count}: acc={acc.item()/count}\")\n",
        "        if epoch_size and count>epoch_size:\n",
        "            break\n",
        "    return total_loss.item()/count, acc.item()/count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3200: acc=0.8096875\n",
            "6400: acc=0.84015625\n",
            "9600: acc=0.8528125\n",
            "12800: acc=0.856875\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.026818861076826735, 0.8601412579957356)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_epoch(net,train_loader,epoch_size=15000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BiGrams、TriGrams 和 N-Grams\n",
        "\n",
        "词袋方法的一个限制是某些词是多词表达的一部分，例如，“热狗”一词与其他上下文中的词“热”和“狗”具有完全不同的含义。 如果我们总是用相同的向量表示单词“hot”和“dog”，它会混淆我们的模型。\n",
        "\n",
        "为了解决这个问题，**N-gram 表示**经常用于文档分类方法，其中每个词、双词或三词的频率是训练分类器的有用特征。 例如，在双元组表示中，除了原始单词之外，我们还会将所有单词对添加到词汇表中。\n",
        "\n",
        "下面是一个如何使用 Scikit Learn 生成词表示的二元词袋的示例：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary:\n",
            " {'i': 7, 'like': 11, 'hot': 4, 'dogs': 2, 'i like': 8, 'like hot': 12, 'hot dogs': 5, 'the': 16, 'dog': 0, 'ran': 14, 'fast': 3, 'the dog': 17, 'dog ran': 1, 'ran fast': 15, 'its': 9, 'outside': 13, 'its hot': 10, 'hot outside': 6}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[1, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
        "corpus = [\n",
        "        'I like hot dogs.',\n",
        "        'The dog ran fast.',\n",
        "        'Its hot outside.',\n",
        "    ]\n",
        "bigram_vectorizer.fit_transform(corpus)\n",
        "print(\"Vocabulary:\\n\",bigram_vectorizer.vocabulary_)\n",
        "bigram_vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "N-gram 方法的主要缺点是词汇量开始增长得非常快。 在实践中，我们需要将 N-gram 表示与一些降维技术相结合，例如 *embeddings*，我们将在下一个单元中讨论。\n",
        "\n",
        "要在我们的 **AG News** 数据集中使用 N-gram 表示，我们需要构建特殊的 ngram 词汇表："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram vocabulary length =  1308844\n"
          ]
        }
      ],
      "source": [
        "counter = collections.Counter()\n",
        "for (label, line) in train_dataset:\n",
        "    l = tokenizer(line)\n",
        "    counter.update(torchtext.data.utils.ngrams_iterator(l,ngrams=2))\n",
        "    \n",
        "bi_vocab = torchtext.vocab.Vocab(counter, min_freq=1)\n",
        "\n",
        "print(\"Bigram vocabulary length = \",len(bi_vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "然后我们可以使用与上面相同的代码来训练分类器，但是，它的内存效率非常低。 在下一个单元中，我们将使用嵌入训练二元分类器。\n",
        "\n",
        "> **注意：** 您只能保留在文本中出现次数超过指定次数的那些 ngram。 这将确保忽略不常见的二元组，并显着降低维度。 为此，将 `min_freq` 参数设置为更高的值，并观察词汇变化的长度。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##词频逆文档频率TF-IDF\n",
        "\n",
        "在 BoW 表示中，单词出现的权重均匀，而与单词本身无关。但是，很明显，与专业术语相比，*a*、*in* 等频繁出现的词对分类的重要性要小得多。事实上，在大多数 NLP 任务中，有些词比其他词更相关。\n",
        "\n",
        "**TF-IDF** 代表**词频-逆文档频率**。它是词袋的一种变体，其中使用浮点值而不是指示单词在文档中出现的二进制 0/1 值，该值与语料库中单词出现的频率有关。\n",
        "\n",
        "更正式地，文档$j$中单词$i$的权重$w_{ij}$定义为：\n",
        "$$\n",
        "w_{ij} = tf_{ij}\\times\\log({N\\over df_i})\n",
        "$$\n",
        "在哪里\n",
        "* $tf_{ij}$是$j$中$i$出现的次数，即我们之前看到的BoW值\n",
        "* $N$ 是集合中的文档数\n",
        "* $df_i$ 是整个集合中包含单词 $i$ 的文档数\n",
        "\n",
        "TF-IDF 值 $w_{ij}$ 与单词在文档中出现的次数成正比增加，并被包含该单词的语料库中的文档数所抵消，这有助于调整某些单词出现的事实比其他人更频繁。例如，如果该词出现在集合中的*每个* 文档中，$df_i=N$ 和 $w_{ij}=0$，这些词将被完全忽略。\n",
        "\n",
        "您可以使用 Scikit Learn 轻松创建文本的 TF-IDF 矢量化："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.43381609, 0.        , 0.43381609, 0.        , 0.65985664,\n",
              "        0.43381609, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
        "vectorizer.fit_transform(corpus)\n",
        "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "然而，即使 TF-IDF 表示为不同的单词提供频率权重，它们也无法表示含义或顺序。 正如著名语言学家 J. R. Firth 在 1935 年所说的那样，“一个词的完整意义总是与语境相关的，没有语境就不能认真研究意义。”。 我们将在后面的单元中学习如何使用语言建模从文本中捕获上下文信息。"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "eec8323f1fd0687cc16c170f83e009bfe267a929289f6367d1c75a585500211d"
    },
    "kernelspec": {
      "display_name": "py37_pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
